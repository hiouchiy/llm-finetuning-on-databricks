{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9315cf2a-ddbb-4ed3-88b7-eaacad3c411d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 17.3 ML LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9385d0-b339-4f54-8961-41f3c846e4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27bcebd0-1ac0-42af-b6dc-b8f09afa39d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import mlflow\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "MODEL_ID = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
    "DATASET_ID = \"bbz662bbz/databricks-dolly-15k-ja-gozaru\"  # License: CC BY-SA 3.0  [oai_citation:1‡Hugging Face](https://huggingface.co/datasets/bbz662bbz/databricks-dolly-15k-ja-gozaru?utm_source=chatgpt.com)\n",
    "\n",
    "# （任意）実験を固定したい場合：自分のWSパスに合わせて変更\n",
    "mlflow.set_experiment(\"/Workspace/Users/hiroshi.ouchiyama@databricks.com/nemotron_nano_gozaru_lora\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def build_user_text(ex):\n",
    "    inst = (ex.get(\"instruction\") or \"\").strip()\n",
    "    inp = (ex.get(\"input\") or \"\").strip()\n",
    "    return f\"{inst}\\n\\n[入力]\\n{inp}\" if inp else inst\n",
    "\n",
    "def to_text(ex):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
    "        {\"role\": \"user\", \"content\": build_user_text(ex)},\n",
    "        {\"role\": \"assistant\", \"content\": (ex.get(\"output\") or \"\").strip()},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "ds = load_dataset(DATASET_ID, split=\"train\")  # 15k rows  [oai_citation:2‡Hugging Face](https://huggingface.co/datasets/bbz662bbz/databricks-dolly-15k-ja-gozaru?utm_source=chatgpt.com)\n",
    "ds = ds.map(to_text, remove_columns=ds.column_names)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "# model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "model = get_peft_model(model, lora)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "if getattr(model, \"generation_config\", None) is not None:\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75f4ad6-2e97-4ba9-87e5-7f8bd4dbad4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "output_dir = \"/local_disk0/nemotron_nano_9b_gozaru_lora\"\n",
    "adapter_dir = \"/local_disk0/nemotron_nano_9b_gozaru_lora_adapter\"\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    report_to=[\"mlflow\"],\n",
    "\n",
    "    # ← ここが “旧SFTTrainer引数” ではなく “SFTConfig側” へ\n",
    "    max_length=2048,\n",
    "    packing=False,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,  # tokenizer 引数は新しめTRLでは processing_class\n",
    "    train_dataset=ds,            # ds は {\"text\": \"...\"} 形式でOK  [oai_citation:1‡Hugging Face](https://huggingface.co/docs/trl/en/sft_trainer)\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35608769-2766-4fcc-a096-a54ea744a5b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"nemotron_nano_9b_gozaru_lora_sft\"):\n",
    "    # ---- params / tags ----\n",
    "    mlflow.set_tag(\"base_model\", MODEL_ID)\n",
    "    mlflow.set_tag(\"dataset\", DATASET_ID)\n",
    "    mlflow.set_tag(\"task\", \"SFT + LoRA\")\n",
    "    mlflow.log_params({\n",
    "        \"lora_r\": lora.r,\n",
    "        \"lora_alpha\": lora.lora_alpha,\n",
    "        \"lora_dropout\": lora.lora_dropout,\n",
    "        \"max_seq_length\": 2048,\n",
    "        \"packing\": True,\n",
    "        \"torch\": torch.__version__,\n",
    "    })\n",
    "    # TrainingArguments は数が多いので必要なものだけ抜粋\n",
    "    mlflow.log_params({\n",
    "        \"epochs\": args.num_train_epochs,\n",
    "        \"per_device_train_batch_size\": args.per_device_train_batch_size,\n",
    "        \"grad_accum\": args.gradient_accumulation_steps,\n",
    "        \"lr\": args.learning_rate,\n",
    "        \"warmup_ratio\": args.warmup_ratio,\n",
    "        \"scheduler\": args.lr_scheduler_type,\n",
    "        \"bf16\": args.bf16,\n",
    "        \"optim\": args.optim,\n",
    "    })\n",
    "\n",
    "    # ---- train (loss等は report_to=['mlflow'] で自動記録されます) ----\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # 念のため最終lossなどを手動でも残す（環境差で自動が効かない場合の保険）\n",
    "    metrics = train_result.metrics or {}\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, (int, float)):\n",
    "            mlflow.log_metric(k, float(v))\n",
    "\n",
    "    # ---- save + artifacts ----\n",
    "    trainer.model.save_pretrained(adapter_dir)\n",
    "    tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "    # LoRAアダプタ（軽量）をアーティファクトとして保存\n",
    "    mlflow.log_artifacts(adapter_dir, artifact_path=\"lora_adapter\")\n",
    "    # 学習の出力（checkpoint等）も必要なら\n",
    "    mlflow.log_artifacts(output_dir, artifact_path=\"trainer_output\")\n",
    "\n",
    "    # 再現性のため：pip freeze も残す（任意）\n",
    "    import subprocess, sys, textwrap\n",
    "    freeze = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n",
    "    with open(\"/tmp/pip_freeze.txt\", \"w\") as f:\n",
    "        f.write(freeze)\n",
    "    mlflow.log_artifact(\"/tmp/pip_freeze.txt\", artifact_path=\"env\")\n",
    "\n",
    "print(\"✅ done\")\n",
    "print(\"adapter_dir:\", adapter_dir)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02.SFT+LoRA on Single GPU with HF TRL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
