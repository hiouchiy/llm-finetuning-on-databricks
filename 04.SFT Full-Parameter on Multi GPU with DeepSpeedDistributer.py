# Databricks notebook source
# MAGIC %md
# MAGIC # 17.3 ML LTS

# COMMAND ----------

# MAGIC %pip install -r requirements.txt
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

import os
import sys
from pyspark.ml.deepspeed.deepspeed_distributor import DeepspeedTorchDistributor
import torch

os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

GPU_NUM = torch.cuda.device_count()

# ========================================
# Databricksèªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã¦ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
# ========================================
def get_databricks_credentials():
    """ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç’°å¢ƒã‹ã‚‰Databricksèªè¨¼æƒ…å ±ã‚’å–å¾—"""
    try:
        # Databricksãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç’°å¢ƒã§ã¯ dbutils ãŒåˆ©ç”¨å¯èƒ½
        from dbruntime.databricks_repl_context import get_context
        context = get_context()
        
        host = context.apiUrl  # ä¾‹: "https://xxx.cloud.databricks.com"
        token = context.apiToken
        
        return host, token
    except Exception as e:
        print(f"Warning: Could not get credentials automatically: {e}")
        return None, None

# èªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã¦ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
DATABRICKS_HOST, DATABRICKS_TOKEN = get_databricks_credentials()

if DATABRICKS_HOST and DATABRICKS_TOKEN:
    os.environ["DATABRICKS_HOST"] = DATABRICKS_HOST
    os.environ["DATABRICKS_TOKEN"] = DATABRICKS_TOKEN
    print(f"âœ… Set DATABRICKS_HOST: {DATABRICKS_HOST}")
else:
    print("âš ï¸ Could not retrieve Databricks credentials")

# ========================================
# æ¨™æº–å‡ºåŠ›ã‚’ç”»é¢ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«åŒæ™‚å‡ºåŠ›ã™ã‚‹ã‚¯ãƒ©ã‚¹
# ========================================
class TeeLogger:
    """æ¨™æº–å‡ºåŠ›ã‚’ç”»é¢ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«åŒæ™‚ã«æ›¸ãè¾¼ã‚€"""
    def __init__(self, log_file, mode='a'):
        self.terminal = sys.stdout
        self.log = open(log_file, mode, buffering=1)  # è¡Œãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
    
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()  # â˜… å³åº§ã«ãƒ‡ã‚£ã‚¹ã‚¯ã«æ›¸ãè¾¼ã¿
    
    def flush(self):
        self.terminal.flush()
        self.log.flush()
    
    def close(self):
        self.log.close()


# ========================================
# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°ï¼ˆMLflowã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å¾©æ´»ï¼‰
# ========================================
def train_nemotron_fullft():
    import os
    import sys
    import torch
    import torch.distributed as dist
    import mlflow
    from datasets import load_dataset
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback
    from trl import SFTTrainer
    
    # 1) local_rank ã‚’å–ã‚‹ï¼ˆtorchrun/Distributor ãŒç’°å¢ƒå¤‰æ•°ã§æ¸¡ã™ï¼‰
    local_rank = int(os.environ.get("LOCAL_RANK", "0"))

    # 2) ã¾ãš â€œã“ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒä½¿ã†GPUâ€ ã‚’å›ºå®šï¼ˆã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼‰
    torch.cuda.set_device(local_rank)
    
    # DDPåˆæœŸåŒ–
    if not dist.is_initialized():
        dist.init_process_group(backend="nccl")
    # local_rank = int(os.environ["LOCAL_RANK"])
    # torch.cuda.set_device(local_rank)
    os.environ["NCCL_P2P_DISABLE"] = "1"

    import os
    import torch
    import torch.distributed as dist
    import mlflow
    from datasets import load_dataset
    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback
    from trl import SFTTrainer, SFTConfig
    
    # DDPåˆæœŸåŒ–
    # dist.init_process_group(backend="nccl")
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    os.environ["NCCL_P2P_DISABLE"] = "1"
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰MLflow Run IDã‚’å–å¾—
    mlflow_run_id = os.environ.get("MLFLOW_RUN_ID")

    # â˜… rank 0ã®ã¿ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
    log_file_path = "/local_disk0/training_output.log"
    if local_rank == 0:
        # æ¨™æº–å‡ºåŠ›ã‚’ç”»é¢ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å²
        tee = TeeLogger(log_file_path, mode='w')  # 'w'ã§æ–°è¦ä½œæˆ
        sys.stdout = tee
        sys.stderr = tee  # ã‚¨ãƒ©ãƒ¼ã‚‚è¨˜éŒ²ã—ãŸã„å ´åˆ
        print(f"ğŸ“ Logging all output to {log_file_path}")
    
    # ========================================
    # MLflowã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆrank 0ã®ã¿ï¼‰
    # ========================================
    class MLflowLoggingCallback(TrainerCallback):
        def __init__(self, run_id, log_file):
            self.run_id = run_id
            self.log_file = log_file
            self.local_rank = int(os.environ.get("LOCAL_RANK", 0))
            self.last_log_time = None
            self.upload_threads = []
        
        def on_log(self, args, state, control, logs=None, **kwargs):
            if self.local_rank == 0 and logs and self.run_id:
                try:
                    import time
                    current_time = time.time()
                    
                    if self.last_log_time is not None:
                        elapsed = current_time - self.last_log_time
                        time_per_step = elapsed / args.logging_steps
                        logs["time_per_step"] = time_per_step
                    
                    self.last_log_time = current_time
                    
                    with mlflow.start_run(run_id=self.run_id):
                        for key, value in logs.items():
                            if isinstance(value, (int, float)):
                                mlflow.log_metric(key, value, step=state.global_step)
                        
                        # â˜… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å®šæœŸçš„ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                        if state.global_step % 100 == 0:  # 100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨
                            mlflow.log_artifact(self.log_file, artifact_path="logs")
                            print(f"ğŸ“¤ Uploaded training log (step {state.global_step})")
                
                except Exception as e:
                    print(f"Warning: MLflow logging failed at step {state.global_step}: {e}")
        
        def on_save(self, args, state, control, **kwargs):
            """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜æ™‚ã«éåŒæœŸã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
            if self.local_rank == 0 and self.run_id:
                import threading
                import os
                
                checkpoint_folder = f"checkpoint-{state.global_step}"
                checkpoint_path = os.path.join(args.output_dir, checkpoint_folder)
                
                if not os.path.exists(checkpoint_path):
                    return
                
                def upload_checkpoint():
                    try:
                        import mlflow
                        with mlflow.start_run(run_id=self.run_id):
                            print(f"ğŸ“¤ Uploading {checkpoint_folder} to MLflow (async)...")
                            mlflow.log_artifacts(
                                checkpoint_path,
                                artifact_path=f"checkpoints/{checkpoint_folder}"
                            )
                            print(f"âœ… {checkpoint_folder} uploaded to MLflow")
                    except Exception as e:
                        print(f"âŒ Checkpoint upload failed for {checkpoint_folder}: {e}")
                
                thread = threading.Thread(target=upload_checkpoint, daemon=False)
                thread.start()
                self.upload_threads.append(thread)
                self.upload_threads = [t for t in self.upload_threads if t.is_alive()]
        
        def on_train_end(self, args, state, control, **kwargs):
            """å­¦ç¿’çµ‚äº†æ™‚ã«å…¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å¾…æ©Ÿï¼‹æœ€çµ‚ãƒ­ã‚°ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
            if self.local_rank == 0:
                if self.upload_threads:
                    print(f"â³ Waiting for {len(self.upload_threads)} uploads to complete...")
                    for thread in self.upload_threads:
                        thread.join()
                    print("âœ… All checkpoint uploads completed")
                
                # â˜… æœ€çµ‚ãƒ­ã‚°ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                if self.run_id:
                    try:
                        import mlflow
                        with mlflow.start_run(run_id=self.run_id):
                            mlflow.log_artifact(self.log_file, artifact_path="logs")
                            print(f"âœ… Final training log uploaded to MLflow")
                    except Exception as e:
                        print(f"Warning: Final log upload failed: {e}")
        
        def on_evaluate(self, args, state, control, metrics=None, **kwargs):
            if self.local_rank == 0 and metrics and self.run_id:
                try:
                    with mlflow.start_run(run_id=self.run_id):
                        for key, value in metrics.items():
                            if isinstance(value, (int, float)):
                                mlflow.log_metric(f"eval_{key}", value, step=state.global_step)
                except Exception as e:
                    print(f"Warning: MLflow eval logging failed: {e}")

    
    MODEL_ID = "nvidia/NVIDIA-Nemotron-Nano-9B-v2"
    DATASET_ID = "bbz662bbz/databricks-dolly-15k-ja-gozaru"
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    def build_user_text(ex):
        inst = (ex.get("instruction") or "").strip()
        inp = (ex.get("input") or "").strip()
        return f"{inst}\n\n[å…¥åŠ›]\n{inp}" if inp else inst
    
    def to_text(ex):
        messages = [
            {"role": "system", "content": "/no_think"},
            {"role": "user", "content": build_user_text(ex)},
            {"role": "assistant", "content": (ex.get("output") or "").strip()},
        ]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        return {"text": text}
    
    ds = load_dataset(DATASET_ID, split="train")
    ds = ds.map(to_text, remove_columns=ds.column_names)
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    model = model.to(local_rank)
    model.config.use_cache = False
    model.config.pad_token_id = tokenizer.pad_token_id
    if getattr(model, "generation_config", None) is not None:
        model.generation_config.pad_token_id = tokenizer.pad_token_id
    
    output_dir = "/local_disk0/nemotron_nano_9b_gozaru_fullft"
    

    # ===============================
    # DeepSpeed ZeRO-3 config (CPU offloadãªã—)
    # - ZeRO Stage 3
    # - offload_param / offload_optimizer ã¯è¨­å®šã—ãªã„ï¼ˆ=CPU offloadãªã—ï¼‰
    # - stage3_gather_16bit_weights_on_model_save ã‚’æœ‰åŠ¹åŒ–ï¼ˆä¿å­˜æ™‚ã®å…¨GPUé›†ç´„ï¼‰
    # ===============================
    import json
    # ds_config = {
    #     "train_micro_batch_size_per_gpu": "auto",
    #     "gradient_accumulation_steps": "auto",
    #     "bf16": {"enabled": True},
    #     "zero_optimization": {
    #         "stage": 3,
    #         "overlap_comm": True,
    #         "contiguous_gradients": True,
    #         "reduce_scatter": True,
    #         "stage3_gather_16bit_weights_on_model_save": True,
    #         # "zero_allow_untested_optimizer": True
    #     }
    # }
    
    # ds_config_path = "/tmp/ds_zero3_no_offload.json"
    
    ds_config = {
        "wall_clock_breakdown": True,
        "train_micro_batch_size_per_gpu": "auto",
        "gradient_accumulation_steps": "auto",
        "bf16": {"enabled": True},
        "zero_optimization": {
            "stage": 2,  # Stage 3 â†’ 2 ã«å¤‰æ›´
            "offload_optimizer": {
                "device": "none"  # âœ… è¾æ›¸å½¢å¼ã§æŒ‡å®š
            },
            "overlap_comm": False,
            "contiguous_gradients": True,
            "reduce_bucket_size": 2e8,
            "allgather_bucket_size": 2e8
        }
    }
    
    ds_config_path = "/tmp/ds_zero2_no_offload.json"
    with open(ds_config_path, "w") as f:
        json.dump(ds_config, f)
    args = SFTConfig(
        output_dir=output_dir,
        num_train_epochs=1,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        warmup_ratio=0.03,
        lr_scheduler_type="cosine",
        logging_steps=1,
        save_steps=200,
        save_total_limit=2,
        bf16=True,
        deepspeed=ds_config_path,
        optim="adamw_torch_fused",
        report_to=[],
        max_length=2048,
        packing=False,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        # ddp_find_unused_parameters=False,
    )
    
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ 
    callbacks = []
    if mlflow_run_id:
        callbacks.append(MLflowLoggingCallback(run_id=mlflow_run_id, log_file=log_file_path))
    
    trainer = SFTTrainer(
        model=model,
        processing_class=tokenizer,
        train_dataset=ds,
        args=args,
        callbacks=callbacks,
    )
    
    # å­¦ç¿’å®Ÿè¡Œ
    train_result = trainer.train()
    
    # rank 0ã®ã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    if local_rank == 0:
        trainer.model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        print("âœ… Training done")
        print("model_dir:", output_dir)

        # â˜… æ¨™æº–å‡ºåŠ›ã‚’å…ƒã«æˆ»ã™
        sys.stdout = tee.terminal
        sys.stderr = tee.terminal
        tee.close()
    
    # dist.destroy_process_group()
    
    if local_rank == 0:
        return {"model_dir": output_dir, "log_file": log_file_path, "metrics": train_result.metrics}
    return None


# ========================================
# ãƒ¡ã‚¤ãƒ³ã‚»ãƒ«ï¼šMLflow Runä½œæˆ â†’ å­¦ç¿’å®Ÿè¡Œ
# ========================================
import mlflow

username = spark.sql("SELECT current_user()").collect()[0][0]
MLFLOW_EXPERIMENT_NAME = f"/Workspace/Users/{username}/nemotron_nano_gozaru_fullft"

if mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME) is None:
    mlflow.create_experiment(name=MLFLOW_EXPERIMENT_NAME)
mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)

# MLflow configuration
os.environ["HF_MLFLOW_LOG_ARTIFACTS"] = "TRUE"  # enable MLflow logging of artifacts
os.environ["MLFLOW_EXPERIMENT_NAME"] = MLFLOW_EXPERIMENT_NAME

with mlflow.start_run(run_name="nemotron_nano_9b_gozaru_fullft_sft") as run:
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
    mlflow.set_tag("base_model", "nvidia/NVIDIA-Nemotron-Nano-9B-v2")
    mlflow.set_tag("dataset", "bbz662bbz/databricks-dolly-15k-ja-gozaru")
    mlflow.set_tag("task", "SFT full-parameter finetuning (DeepSpeed ZeRO-3)")
    mlflow.log_params({        "epochs": 1,
        "per_device_train_batch_size": 1,
        "grad_accum": 8,
        "lr": 2e-4,
        "num_gpus": GPU_NUM,
    })
    
    # Run IDã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆå­ãƒ—ãƒ­ã‚»ã‚¹ã«å¼•ãç¶™ãŒã‚Œã‚‹ï¼‰
    os.environ["MLFLOW_RUN_ID"] = run.info.run_id
    
    # DDPå­¦ç¿’å®Ÿè¡Œ
    distributor = DeepspeedTorchDistributor(
        numGpus=GPU_NUM,
        nnodes=1,
        localMode=True,
        useGpu=True,
        deepspeedConfig=None  # ZeROè¨­å®šã¯SFTConfig(deepspeed=...)å´ã§æŒ‡å®š
    )
    
    result = distributor.run(train_nemotron_fullft)
    
    # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜
    if result and "model_dir" in result:
        mlflow.log_artifacts(result["model_dir"], artifact_path="model")

print("âœ… All done!")

# COMMAND ----------


