{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91606c4d-1ed0-48f0-a34d-6f273389af93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 17.3 ML LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9385d0-b339-4f54-8961-41f3c846e4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27bcebd0-1ac0-42af-b6dc-b8f09afa39d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import torch\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "GPU_NUM = torch.cuda.device_count()\n",
    "\n",
    "# ========================================\n",
    "# Databricksèªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã¦ç’°å¢ƒå¤‰æ•°ã«è¨­å®š\n",
    "# ========================================\n",
    "def get_databricks_credentials():\n",
    "    \"\"\"ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç’°å¢ƒã‹ã‚‰Databricksèªè¨¼æƒ…å ±ã‚’å–å¾—\"\"\"\n",
    "    try:\n",
    "        # Databricksãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç’°å¢ƒã§ã¯ dbutils ãŒåˆ©ç”¨å¯èƒ½\n",
    "        from dbruntime.databricks_repl_context import get_context\n",
    "        context = get_context()\n",
    "        \n",
    "        host = context.apiUrl  # ä¾‹: \"https://xxx.cloud.databricks.com\"\n",
    "        token = context.apiToken\n",
    "        \n",
    "        return host, token\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not get credentials automatically: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# èªè¨¼æƒ…å ±ã‚’å–å¾—ã—ã¦ç’°å¢ƒå¤‰æ•°ã«è¨­å®š\n",
    "DATABRICKS_HOST, DATABRICKS_TOKEN = get_databricks_credentials()\n",
    "\n",
    "if DATABRICKS_HOST and DATABRICKS_TOKEN:\n",
    "    os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "    os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN\n",
    "    print(f\"âœ… Set DATABRICKS_HOST: {DATABRICKS_HOST}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not retrieve Databricks credentials\")\n",
    "\n",
    "# ========================================\n",
    "# æ¨™æº–å‡ºåŠ›ã‚’ç”»é¢ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«åŒæ™‚å‡ºåŠ›ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "# ========================================\n",
    "class TeeLogger:\n",
    "    \"\"\"æ¨™æº–å‡ºåŠ›ã‚’ç”»é¢ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«åŒæ™‚ã«æ›¸ãè¾¼ã‚€\"\"\"\n",
    "    def __init__(self, log_file, mode='a'):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(log_file, mode, buffering=1)  # è¡Œãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°\n",
    "    \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush()  # â˜… å³åº§ã«ãƒ‡ã‚£ã‚¹ã‚¯ã«æ›¸ãè¾¼ã¿\n",
    "    \n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "    \n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°ï¼ˆMLflowã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å¾©æ´»ï¼‰\n",
    "# ========================================\n",
    "def train_nemotron_lora():\n",
    "    import os\n",
    "    import sys\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    import mlflow\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    \n",
    "    # DDPåˆæœŸåŒ–\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    import mlflow\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    \n",
    "    # DDPåˆæœŸåŒ–\n",
    "    # dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "    \n",
    "    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰MLflow Run IDã‚’å–å¾—\n",
    "    mlflow_run_id = os.environ.get(\"MLFLOW_RUN_ID\")\n",
    "\n",
    "    # â˜… rank 0ã®ã¿ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²\n",
    "    log_file_path = \"/local_disk0/training_output.log\"\n",
    "    if local_rank == 0:\n",
    "        # æ¨™æº–å‡ºåŠ›ã‚’ç”»é¢ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å²\n",
    "        tee = TeeLogger(log_file_path, mode='w')  # 'w'ã§æ–°è¦ä½œæˆ\n",
    "        sys.stdout = tee\n",
    "        sys.stderr = tee  # ã‚¨ãƒ©ãƒ¼ã‚‚è¨˜éŒ²ã—ãŸã„å ´åˆ\n",
    "        print(f\"ğŸ“ Logging all output to {log_file_path}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # MLflowã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆrank 0ã®ã¿ï¼‰\n",
    "    # ========================================\n",
    "    class MLflowLoggingCallback(TrainerCallback):\n",
    "        def __init__(self, run_id, log_file):\n",
    "            self.run_id = run_id\n",
    "            self.log_file = log_file\n",
    "            self.local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "            self.last_log_time = None\n",
    "            self.upload_threads = []\n",
    "        \n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            if self.local_rank == 0 and logs and self.run_id:\n",
    "                try:\n",
    "                    import time\n",
    "                    current_time = time.time()\n",
    "                    \n",
    "                    if self.last_log_time is not None:\n",
    "                        elapsed = current_time - self.last_log_time\n",
    "                        time_per_step = elapsed / args.logging_steps\n",
    "                        logs[\"time_per_step\"] = time_per_step\n",
    "                    \n",
    "                    self.last_log_time = current_time\n",
    "                    \n",
    "                    with mlflow.start_run(run_id=self.run_id):\n",
    "                        for key, value in logs.items():\n",
    "                            if isinstance(value, (int, float)):\n",
    "                                mlflow.log_metric(key, value, step=state.global_step)\n",
    "                        \n",
    "                        # â˜… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å®šæœŸçš„ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "                        if state.global_step % 100 == 0:  # 100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨\n",
    "                            mlflow.log_artifact(self.log_file, artifact_path=\"logs\")\n",
    "                            print(f\"ğŸ“¤ Uploaded training log (step {state.global_step})\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: MLflow logging failed at step {state.global_step}: {e}\")\n",
    "        \n",
    "        def on_save(self, args, state, control, **kwargs):\n",
    "            \"\"\"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜æ™‚ã«éåŒæœŸã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\"\"\"\n",
    "            if self.local_rank == 0 and self.run_id:\n",
    "                import threading\n",
    "                import os\n",
    "                \n",
    "                checkpoint_folder = f\"checkpoint-{state.global_step}\"\n",
    "                checkpoint_path = os.path.join(args.output_dir, checkpoint_folder)\n",
    "                \n",
    "                if not os.path.exists(checkpoint_path):\n",
    "                    return\n",
    "                \n",
    "                def upload_checkpoint():\n",
    "                    try:\n",
    "                        import mlflow\n",
    "                        with mlflow.start_run(run_id=self.run_id):\n",
    "                            print(f\"ğŸ“¤ Uploading {checkpoint_folder} to MLflow (async)...\")\n",
    "                            mlflow.log_artifacts(\n",
    "                                checkpoint_path,\n",
    "                                artifact_path=f\"checkpoints/{checkpoint_folder}\"\n",
    "                            )\n",
    "                            print(f\"âœ… {checkpoint_folder} uploaded to MLflow\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"âŒ Checkpoint upload failed for {checkpoint_folder}: {e}\")\n",
    "                \n",
    "                thread = threading.Thread(target=upload_checkpoint, daemon=False)\n",
    "                thread.start()\n",
    "                self.upload_threads.append(thread)\n",
    "                self.upload_threads = [t for t in self.upload_threads if t.is_alive()]\n",
    "        \n",
    "        def on_train_end(self, args, state, control, **kwargs):\n",
    "            \"\"\"å­¦ç¿’çµ‚äº†æ™‚ã«å…¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å¾…æ©Ÿï¼‹æœ€çµ‚ãƒ­ã‚°ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\"\"\"\n",
    "            if self.local_rank == 0:\n",
    "                if self.upload_threads:\n",
    "                    print(f\"â³ Waiting for {len(self.upload_threads)} uploads to complete...\")\n",
    "                    for thread in self.upload_threads:\n",
    "                        thread.join()\n",
    "                    print(\"âœ… All checkpoint uploads completed\")\n",
    "                \n",
    "                # â˜… æœ€çµ‚ãƒ­ã‚°ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "                if self.run_id:\n",
    "                    try:\n",
    "                        import mlflow\n",
    "                        with mlflow.start_run(run_id=self.run_id):\n",
    "                            mlflow.log_artifact(self.log_file, artifact_path=\"logs\")\n",
    "                            print(f\"âœ… Final training log uploaded to MLflow\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Final log upload failed: {e}\")\n",
    "        \n",
    "        def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "            if self.local_rank == 0 and metrics and self.run_id:\n",
    "                try:\n",
    "                    with mlflow.start_run(run_id=self.run_id):\n",
    "                        for key, value in metrics.items():\n",
    "                            if isinstance(value, (int, float)):\n",
    "                                mlflow.log_metric(f\"eval_{key}\", value, step=state.global_step)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: MLflow eval logging failed: {e}\")\n",
    "\n",
    "    \n",
    "    MODEL_ID = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\n",
    "    DATASET_ID = \"bbz662bbz/databricks-dolly-15k-ja-gozaru\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    def build_user_text(ex):\n",
    "        inst = (ex.get(\"instruction\") or \"\").strip()\n",
    "        inp = (ex.get(\"input\") or \"\").strip()\n",
    "        return f\"{inst}\\n\\n[å…¥åŠ›]\\n{inp}\" if inp else inst\n",
    "    \n",
    "    def to_text(ex):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
    "            {\"role\": \"user\", \"content\": build_user_text(ex)},\n",
    "            {\"role\": \"assistant\", \"content\": (ex.get(\"output\") or \"\").strip()},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        return {\"text\": text}\n",
    "    \n",
    "    ds = load_dataset(DATASET_ID, split=\"train\")\n",
    "    ds = ds.map(to_text, remove_columns=ds.column_names)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = model.to(local_rank)\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    lora = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=\"all-linear\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    if getattr(model, \"generation_config\", None) is not None:\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    output_dir = \"/local_disk0/nemotron_nano_9b_gozaru_lora\"\n",
    "    adapter_dir = \"/local_disk0/nemotron_nano_9b_gozaru_lora_adapter\"\n",
    "    \n",
    "    args = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=10,\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        report_to=[],\n",
    "        max_length=2048,\n",
    "        packing=False,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        # ddp_find_unused_parameters=False,\n",
    "    )\n",
    "    \n",
    "    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ \n",
    "    callbacks = []\n",
    "    if mlflow_run_id:\n",
    "        callbacks.append(MLflowLoggingCallback(run_id=mlflow_run_id, log_file=log_file_path))\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=ds,\n",
    "        args=args,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    # å­¦ç¿’å®Ÿè¡Œ\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # rank 0ã®ã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "    if local_rank == 0:\n",
    "        trainer.model.save_pretrained(adapter_dir)\n",
    "        tokenizer.save_pretrained(adapter_dir)\n",
    "        print(\"âœ… Training done\")\n",
    "        print(\"adapter_dir:\", adapter_dir)\n",
    "\n",
    "        # â˜… æ¨™æº–å‡ºåŠ›ã‚’å…ƒã«æˆ»ã™\n",
    "        sys.stdout = tee.terminal\n",
    "        sys.stderr = tee.terminal\n",
    "        tee.close()\n",
    "    \n",
    "    # dist.destroy_process_group()\n",
    "    \n",
    "    if local_rank == 0:\n",
    "        return {\"adapter_dir\": adapter_dir, \"log_file\": log_file_path, \"metrics\": train_result.metrics}\n",
    "    return None\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ãƒ¡ã‚¤ãƒ³ã‚»ãƒ«ï¼šMLflow Runä½œæˆ â†’ å­¦ç¿’å®Ÿè¡Œ\n",
    "# ========================================\n",
    "import mlflow\n",
    "\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "MLFLOW_EXPERIMENT_NAME = f\"/Workspace/Users/{username}/nemotron_nano_gozaru_lora\"\n",
    "\n",
    "if mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME) is None:\n",
    "    mlflow.create_experiment(name=MLFLOW_EXPERIMENT_NAME)\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# MLflow configuration\n",
    "os.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"] = \"TRUE\"  # enable MLflow logging of artifacts\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = MLFLOW_EXPERIMENT_NAME\n",
    "\n",
    "with mlflow.start_run(run_name=\"nemotron_nano_9b_gozaru_lora_sft\") as run:\n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²\n",
    "    mlflow.set_tag(\"base_model\", \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\n",
    "    mlflow.set_tag(\"dataset\", \"bbz662bbz/databricks-dolly-15k-ja-gozaru\")\n",
    "    mlflow.set_tag(\"task\", \"SFT + LoRA DDP\")\n",
    "    mlflow.log_params({\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"epochs\": 1,\n",
    "        \"per_device_train_batch_size\": 1,\n",
    "        \"grad_accum\": 8,\n",
    "        \"lr\": 2e-4,\n",
    "        \"num_gpus\": GPU_NUM,\n",
    "    })\n",
    "    \n",
    "    # Run IDã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šï¼ˆå­ãƒ—ãƒ­ã‚»ã‚¹ã«å¼•ãç¶™ãŒã‚Œã‚‹ï¼‰\n",
    "    os.environ[\"MLFLOW_RUN_ID\"] = run.info.run_id\n",
    "    \n",
    "    # DDPå­¦ç¿’å®Ÿè¡Œ\n",
    "    distributor = TorchDistributor(\n",
    "        num_processes=GPU_NUM,\n",
    "        local_mode=True,\n",
    "        use_gpu=True\n",
    "    )\n",
    "    \n",
    "    result = distributor.run(train_nemotron_lora)\n",
    "    \n",
    "    # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜\n",
    "    if result and \"adapter_dir\" in result:\n",
    "        mlflow.log_artifacts(result[\"adapter_dir\"], artifact_path=\"lora_adapter\")\n",
    "\n",
    "print(\"âœ… All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3969c176-3590-4d89-a918-5764d56ccc92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03.SFT+LoRA on Multi GPU with TorchDistributer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
